# データセット

日本語文章のかな3連接のデータが欲しいので、探してみます。

→ひとまず岡さんのWikipediaのデータを使うことにします。

## Wikipediaのデータについて

少し分析してみます。

wikipedia.hiragana-ized.3gram.txt

- 総出現回数
- エントリ数 91774行
- 合計がx%を越える行数を5%刻みで、行の割合も示しながら(イメージ: 合計50% 10000行（10%）)

```text
$ bun run scripts/analyze-3gram.ts
ファイル: dataset/wikipedia.hiragana-ized.3gram.txt
総出現回数: 1,094,322,391
エントリ数: 91,774

累積カバー率ごとの行数（5%刻み）:
合計5%: 38行（0.04%）
合計10%: 128行（0.14%）
合計15%: 269行（0.29%）
合計20%: 470行（0.51%）
合計25%: 744行（0.81%）
合計30%: 1,106行（1.21%）
合計35%: 1,577行（1.72%）
合計40%: 2,177行（2.37%）
合計45%: 2,944行（3.21%）
合計50%: 3,924行（4.28%）
合計55%: 5,169行（5.63%）
合計60%: 6,763行（7.37%）
合計65%: 8,820行（9.61%）
合計70%: 11,503行（12.53%）
合計75%: 15,066行（16.42%）
合計80%: 19,962行（21.75%）
合計85%: 26,932行（29.35%）
合計90%: 37,457行（40.81%）
合計95%: 55,092行（60.03%）
合計100%: 91,774行（100.00%）
```

次は打鍵数ごとの割合を調べる。8打鍵とかあってびっくりしたけど、拗音や外来音の小書きを0文字としてカウントしているので、例えば「、ぎょぎょ」みたいなのが合って2+3+3=8打鍵になる。そういう理由もあって4打鍵がボリュームゾーンになっている。

```
$ bun run scripts/analyze-3gram-keystrokes.ts
未対応の文字をスキップ: ゖ
未対応の文字をスキップ: ゖ
未対応の文字をスキップ: ゖ
ファイル: dataset/wikipedia.hiragana-ized.3gram.txt
対象エントリ数: 91774 / 91774 (スキップ: 0)
総出現回数: 1,094,322,391

打鍵数別分布:
2打鍵: 3,446回（0.00%）, 2行（0.00%）
3打鍵: 360,651,759回（32.96%）, 17,535行（19.11%）
4打鍵: 517,795,336回（47.32%）, 44,261行（48.23%）
5打鍵: 193,009,062回（17.64%）, 26,043行（28.38%）
6打鍵: 22,091,949回（2.02%）, 3,752行（4.09%）
7打鍵: 754,662回（0.07%）, 174行（0.19%）
8打鍵: 14,002回（0.00%）, 6行（0.01%）
9打鍵: 2,175回（0.00%）, 1行（0.00%）
```

1~3gramをまとめて配置しているものがあるので、ここから自分の目的に合った3-gramを抽出して使うのがよさそう。

ひらがなの小さいけだけ除外して抽出した。打鍵数を数えたところうまくできてそう。とりあえずこのデータを使う。全部で78964行。4打鍵が一番多いからもっとうまくできるかもしれない。

```
$ bun run scripts/analyze-3gram-keystrokes.ts dataset/oka-wikipedia-3gram.tsv
ファイル: dataset/oka-wikipedia-3gram.tsv
対象エントリ数: 78964 / 78964 (スキップ: 0)
総出現回数: 1,066,148,043

打鍵数別分布:
3打鍵: 437,519,938回（41.04%）, 18,696行（23.68%）
4打鍵: 480,113,774回（45.03%）, 39,527行（50.06%）
5打鍵: 139,370,045回（13.07%）, 19,126行（24.22%）
6打鍵: 9,144,286回（0.86%）, 1,615行（2.05%）
```

## メモ

### [気軽に使えるngramデータ「日本語ウェブコーパス 2010」を使ってみる - エイエイレトリック](https://eieito.hatenablog.com/entry/2021/06/15/100000)

文字n-gramのデータも公開されている。漢字なのでひらがなに変換する必要がある。

### [大規模日本語 n-gram データの公開 : 一言語学徒のページ](https://ling.exblog.jp/6449935/)

こちらはリンク切れでどこに置かれているか分からなかった。英語のn-gramはGoogleが公開しているらしい。

「Webから抽出した200億文」とあって、規模がとんでもない。

### [oktopus1959/wikipedia_ja_ngram: 日本語Wikipediaからひらがなのn-gramを抽出しました](https://github.com/oktopus1959/wikipedia_ja_ngram)

Wikipediaからひらがなn-gramを抽出したもの。漢直WSの開発者の岡さんが作成されている。

Wikipediaなのでである調がほとんどでですます調の割合は少ないので、そこを補うデータが必要だろうか。

使用例: [DHSBことのは配列と連接の話｜myomyomyo](https://note.com/myomyomyo4256/n/n96e151037b28#6cd7f67d-fe36-4404-bf51-18fa327825d4)

### [100万字日本語かなn-gramデータ : ローマ字入力でもなく、かな入力でもなく](https://kouy.exblog.jp/9731073/#goog_rewarded)

kouyさんによるもの。

### [【自然言語処理】フリーで使える大規模な日本語テキストコーパス](https://tma15.github.io/blog/2023/01/08/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%83%95%E3%83%AA%E3%83%BC%E3%81%A7%E4%BD%BF%E3%81%88%E3%82%8B%E5%A4%A7%E8%A6%8F%E6%A8%A1%E3%81%AA%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9/)

CC-100、mC4、OSCARの3つのコーパスが紹介されている。どれもCommon Crawlという有志によって集められたWeb上の文章のデータをもとに作成されている。

### [世界で開発が進む大規模言語モデルとは（後編） | 株式会社NTTデータ先端技術](https://www.intellilink.co.jp/column/ai/2022/072800.aspx)

GPTは学習データとして、Japanese CC-100、日本語Wikipedia、Japanese C4（mC4から日本語のみを抜き出したもの）を使用している。
